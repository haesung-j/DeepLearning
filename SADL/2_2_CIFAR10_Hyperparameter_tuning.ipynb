{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93993786",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a106b9",
   "metadata": {},
   "source": [
    "# 1. Train_Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4355cd81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),    # image파일을 0 ~ 1사이의 값을 갖는 Tensor로 변환(0: 검은색)\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5), )])  # 채널별로 0.5를 빼고(-0.5 ~ 0.5), 0.5로 나눔\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "# train val split\n",
    "trainset, valset = torch.utils.data.random_split(trainset, [40000, 10000])\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, \n",
    "                                        shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91c803a",
   "metadata": {},
   "source": [
    "# 2. Define MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "2dcc94f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hid_dim, n_layer, act, dropout, bn=True):\n",
    "        super(MLP,self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layer = n_layer\n",
    "        self.act = act\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.in_dim, self.hid_dim)\n",
    "        self.linears = nn.ModuleList()\n",
    "        self.bns = nn.ModuleList()\n",
    "        for i in range(self.n_layer):    # n_layer: hidden layer의 개수\n",
    "            self.linears.append(nn.Linear(self.hid_dim, self.hid_dim))\n",
    "            if bn:\n",
    "                self.bns.append(nn.BatchNorm1d(hid_dim))\n",
    "        self.fc2 = nn.Linear(self.hid_dim, self.out_dim)\n",
    "        \n",
    "        self.do = nn.Dropout(self.dropout)\n",
    "            \n",
    "        if self.act == 'relu':\n",
    "            self.act = nn.ReLU()\n",
    "        elif self.act == 'leakyrelu':\n",
    "            self.act = nn.LeakyReLU()\n",
    "        else:\n",
    "            raise ValueError(\"Invalid activation function!\")\n",
    "            \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.act(self.fc1(x))\n",
    "        for i in range(len(self.linears)):\n",
    "            x = self.act(self.linears[i](x))\n",
    "            x = self.bns[i](x)\n",
    "            x = self.do(x)\n",
    "        x = self.fc2(x)          # Softmax 마지막 layer는 activation func이 없어야 함\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "19ebcfb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (fc1): Linear(in_features=3072, out_features=200, bias=True)\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "  )\n",
      "  (bns): ModuleList(\n",
      "    (0): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (fc2): Linear(in_features=200, out_features=10, bias=True)\n",
      "  (do): Dropout(p=0.1, inplace=False)\n",
      "  (act): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MLP(32*32*3, 10, 200, 1, 'relu', 0.1, True)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9280e3e",
   "metadata": {},
   "source": [
    "# 3. Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "ca75257d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiments(args):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = MLP(args.in_dim, args.out_dim, args.hid_dim, args.n_layer, args.act, args.dropout, args.bn)\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    if args.opt == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.mm)\n",
    "    elif args.opt == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=args.lr, )\n",
    "    elif args.opt == 'Adagrad':\n",
    "        optimizer = optim.Adagrad(model.parameters(), lr=args.lr)\n",
    "    else:\n",
    "        raise ValueError('Invalid optimizer!')\n",
    "    \n",
    "    \n",
    "    for epoch in range(1, args.epochs+1):\n",
    "\n",
    "        ## === Train === ##\n",
    "        running_loss = 0.0\n",
    "        tr_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.view(-1, 32*32*3)\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward, backward, optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print log\n",
    "            running_loss += loss.item()\n",
    "            tr_loss += loss.item()\n",
    "            if i % 2000 == 1999:    # 2000 mini batches마다 출력\n",
    "                print('[epoch: {}, {}] running_loss: {:.3f}'.format(epoch, i+1, running_loss / 2000))\n",
    "                running_loss = 0.0\n",
    "                \n",
    "        tr_loss = tr_loss / len(trainloader.dataset)\n",
    "                \n",
    "        ## === Validation === ##\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data in valloader:\n",
    "                inputs, labels = data\n",
    "                inputs = inputs.view(-1, 32*32*3)\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()    # batch에 대한 loss\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (preds == labels).sum().item()\n",
    "\n",
    "            val_loss = val_loss / total\n",
    "            val_acc = correct / total * 100\n",
    "       \n",
    "        print('Epoch {}, Train Loss: {:.3f}, Val Loss: {:.3f}, Val Acc: {:.3f}%'.format(\n",
    "            epoch,tr_loss, val_loss, val_acc))\n",
    "                    \n",
    "    ## === Test set === ##\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.view(-1, 32*32*3)\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (preds == labels).sum().item()\n",
    "        test_acc = correct / total * 100\n",
    "        \n",
    "    return tr_loss, val_loss, val_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "1a165e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed= 1212\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "args.in_dim = 3 * 32 * 32\n",
    "args.out_dim = 10\n",
    "args.hid_dim = 100\n",
    "args.n_layer = 2\n",
    "args.act = 'relu'\n",
    "args.dropout = 0.1\n",
    "args.bn = True\n",
    "\n",
    "args.opt = 'Adam'\n",
    "\n",
    "args.lr = 0.01\n",
    "args.mm = 0.9\n",
    "\n",
    "args.epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "63d94b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Start Experiment ========\n",
      "layers: 2, hidden_dim: 50\n",
      "[epoch: 1, 2000] running_loss: 2.055\n",
      "[epoch: 1, 4000] running_loss: 1.792\n",
      "[epoch: 1, 6000] running_loss: 1.722\n",
      "[epoch: 1, 8000] running_loss: 1.669\n",
      "[epoch: 1, 10000] running_loss: 1.635\n",
      "Epoch 1, Train Loss: 0.444, Val Loss: 0.416, Val Acc: 40.550%\n",
      "[epoch: 2, 2000] running_loss: 1.578\n",
      "[epoch: 2, 4000] running_loss: 1.565\n",
      "[epoch: 2, 6000] running_loss: 1.562\n",
      "[epoch: 2, 8000] running_loss: 1.543\n",
      "[epoch: 2, 10000] running_loss: 1.511\n",
      "Epoch 2, Train Loss: 0.388, Val Loss: 0.382, Val Acc: 45.880%\n",
      "(0.3878904568515718, 0.38215211568027735, 45.879999999999995, 46.82)\n",
      "======= End Experiment ========\n",
      "======= Start Experiment ========\n",
      "layers: 2, hidden_dim: 100\n",
      "[epoch: 1, 2000] running_loss: 2.074\n",
      "[epoch: 1, 4000] running_loss: 1.794\n",
      "[epoch: 1, 6000] running_loss: 1.702\n",
      "[epoch: 1, 8000] running_loss: 1.644\n",
      "[epoch: 1, 10000] running_loss: 1.623\n",
      "Epoch 1, Train Loss: 0.442, Val Loss: 0.401, Val Acc: 42.500%\n",
      "[epoch: 2, 2000] running_loss: 1.544\n",
      "[epoch: 2, 4000] running_loss: 1.521\n",
      "[epoch: 2, 6000] running_loss: 1.529\n",
      "[epoch: 2, 8000] running_loss: 1.491\n",
      "[epoch: 2, 10000] running_loss: 1.498\n",
      "Epoch 2, Train Loss: 0.379, Val Loss: 0.372, Val Acc: 47.180%\n",
      "(0.3792131873071194, 0.3718449015468359, 47.18, 47.910000000000004)\n",
      "======= End Experiment ========\n",
      "======= Start Experiment ========\n",
      "layers: 3, hidden_dim: 50\n",
      "[epoch: 1, 2000] running_loss: 2.197\n",
      "[epoch: 1, 4000] running_loss: 1.914\n",
      "[epoch: 1, 6000] running_loss: 1.778\n",
      "[epoch: 1, 8000] running_loss: 1.734\n",
      "[epoch: 1, 10000] running_loss: 1.681\n",
      "Epoch 1, Train Loss: 0.465, Val Loss: 0.426, Val Acc: 37.660%\n",
      "[epoch: 2, 2000] running_loss: 1.632\n",
      "[epoch: 2, 4000] running_loss: 1.616\n",
      "[epoch: 2, 6000] running_loss: 1.611\n",
      "[epoch: 2, 8000] running_loss: 1.566\n",
      "[epoch: 2, 10000] running_loss: 1.560\n",
      "Epoch 2, Train Loss: 0.399, Val Loss: 0.402, Val Acc: 42.520%\n",
      "(0.39926848777905105, 0.40199759683012964, 42.52, 44.15)\n",
      "======= End Experiment ========\n",
      "======= Start Experiment ========\n",
      "layers: 3, hidden_dim: 100\n",
      "[epoch: 1, 2000] running_loss: 2.236\n",
      "[epoch: 1, 4000] running_loss: 1.942\n",
      "[epoch: 1, 6000] running_loss: 1.805\n",
      "[epoch: 1, 8000] running_loss: 1.719\n",
      "[epoch: 1, 10000] running_loss: 1.674\n",
      "Epoch 1, Train Loss: 0.469, Val Loss: 0.416, Val Acc: 39.810%\n",
      "[epoch: 2, 2000] running_loss: 1.614\n",
      "[epoch: 2, 4000] running_loss: 1.586\n",
      "[epoch: 2, 6000] running_loss: 1.569\n",
      "[epoch: 2, 8000] running_loss: 1.561\n",
      "[epoch: 2, 10000] running_loss: 1.523\n",
      "Epoch 2, Train Loss: 0.393, Val Loss: 0.389, Val Acc: 43.650%\n",
      "(0.3926371213413775, 0.38918488656580447, 43.65, 45.21)\n",
      "======= End Experiment ========\n"
     ]
    }
   ],
   "source": [
    "list_var1 = [2, 3]        # layers\n",
    "list_var2 = [50, 100]   # hidden_dim\n",
    "\n",
    "for var1 in list_var1:\n",
    "    for var2 in list_var2:\n",
    "        print('======= Start Experiment ========')\n",
    "        print('layers: {}, hidden_dim: {}'.format(var1, var2))\n",
    "\n",
    "        args.n_layer = var1\n",
    "        args.hid_dim = var2\n",
    "        result = experiments(args)\n",
    "        print(result)\n",
    "        print('======= End Experiment ========')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "1c877b51",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Start Experiment ========\n",
      "Optimizer: Adam, learning_rate: 0.001\n",
      "[epoch: 1, 2000] running_loss: 2.164\n",
      "[epoch: 1, 4000] running_loss: 2.063\n",
      "[epoch: 1, 6000] running_loss: 2.036\n",
      "[epoch: 1, 8000] running_loss: 2.020\n",
      "[epoch: 1, 10000] running_loss: 2.001\n",
      "Epoch 1, Train Loss: 0.514, Val Loss: 0.502, Val Acc: 27.490%\n",
      "[epoch: 2, 2000] running_loss: 1.982\n",
      "[epoch: 2, 4000] running_loss: 1.972\n",
      "[epoch: 2, 6000] running_loss: 1.925\n",
      "[epoch: 2, 8000] running_loss: 1.917\n",
      "[epoch: 2, 10000] running_loss: 1.913\n",
      "Epoch 2, Train Loss: 0.485, Val Loss: 0.480, Val Acc: 31.370%\n",
      "[epoch: 3, 2000] running_loss: 1.898\n",
      "[epoch: 3, 4000] running_loss: 1.873\n",
      "[epoch: 3, 6000] running_loss: 1.874\n",
      "[epoch: 3, 8000] running_loss: 1.860\n",
      "[epoch: 3, 10000] running_loss: 1.843\n",
      "Epoch 3, Train Loss: 0.467, Val Loss: 0.463, Val Acc: 34.130%\n",
      "[epoch: 4, 2000] running_loss: 1.830\n",
      "[epoch: 4, 4000] running_loss: 1.842\n",
      "[epoch: 4, 6000] running_loss: 1.823\n",
      "[epoch: 4, 8000] running_loss: 1.808\n",
      "[epoch: 4, 10000] running_loss: 1.808\n",
      "Epoch 4, Train Loss: 0.456, Val Loss: 0.451, Val Acc: 35.680%\n",
      "[epoch: 5, 2000] running_loss: 1.773\n",
      "[epoch: 5, 4000] running_loss: 1.774\n",
      "[epoch: 5, 6000] running_loss: 1.777\n",
      "[epoch: 5, 8000] running_loss: 1.788\n",
      "[epoch: 5, 10000] running_loss: 1.788\n",
      "Epoch 5, Train Loss: 0.445, Val Loss: 0.447, Val Acc: 36.440%\n",
      "(0.44502831533402204, 0.44660547065138817, 36.44, 37.41)\n",
      "======= End Experiment ========\n",
      "======= Start Experiment ========\n",
      "Optimizer: Adam, learning_rate: 0.01\n",
      "[epoch: 1, 2000] running_loss: 2.200\n",
      "[epoch: 1, 4000] running_loss: 2.163\n",
      "[epoch: 1, 6000] running_loss: 2.150\n",
      "[epoch: 1, 8000] running_loss: 2.103\n",
      "[epoch: 1, 10000] running_loss: 2.050\n",
      "Epoch 1, Train Loss: 0.533, Val Loss: 0.513, Val Acc: 26.010%\n",
      "[epoch: 2, 2000] running_loss: 2.034\n",
      "[epoch: 2, 4000] running_loss: 2.031\n",
      "[epoch: 2, 6000] running_loss: 2.024\n",
      "[epoch: 2, 8000] running_loss: 2.040\n",
      "[epoch: 2, 10000] running_loss: 2.002\n",
      "Epoch 2, Train Loss: 0.507, Val Loss: 0.504, Val Acc: 27.190%\n",
      "[epoch: 3, 2000] running_loss: 1.990\n",
      "[epoch: 3, 4000] running_loss: 1.981\n",
      "[epoch: 3, 6000] running_loss: 1.985\n",
      "[epoch: 3, 8000] running_loss: 1.944\n",
      "[epoch: 3, 10000] running_loss: 1.927\n",
      "Epoch 3, Train Loss: 0.491, Val Loss: 0.491, Val Acc: 30.230%\n",
      "[epoch: 4, 2000] running_loss: 1.934\n",
      "[epoch: 4, 4000] running_loss: 1.912\n",
      "[epoch: 4, 6000] running_loss: 1.891\n",
      "[epoch: 4, 8000] running_loss: 1.905\n",
      "[epoch: 4, 10000] running_loss: 1.892\n",
      "Epoch 4, Train Loss: 0.477, Val Loss: 0.486, Val Acc: 28.640%\n",
      "[epoch: 5, 2000] running_loss: 1.879\n",
      "[epoch: 5, 4000] running_loss: 1.870\n",
      "[epoch: 5, 6000] running_loss: 1.888\n",
      "[epoch: 5, 8000] running_loss: 1.893\n",
      "[epoch: 5, 10000] running_loss: 1.864\n",
      "Epoch 5, Train Loss: 0.470, Val Loss: 0.464, Val Acc: 35.010%\n",
      "(0.46975862384140493, 0.4644759646356106, 35.010000000000005, 35.08)\n",
      "======= End Experiment ========\n",
      "======= Start Experiment ========\n",
      "Optimizer: Adagrad, learning_rate: 0.001\n",
      "[epoch: 1, 2000] running_loss: 2.231\n",
      "[epoch: 1, 4000] running_loss: 2.159\n",
      "[epoch: 1, 6000] running_loss: 2.131\n",
      "[epoch: 1, 8000] running_loss: 2.115\n",
      "[epoch: 1, 10000] running_loss: 2.096\n",
      "Epoch 1, Train Loss: 0.537, Val Loss: 0.523, Val Acc: 24.820%\n",
      "[epoch: 2, 2000] running_loss: 2.070\n",
      "[epoch: 2, 4000] running_loss: 2.054\n",
      "[epoch: 2, 6000] running_loss: 2.057\n",
      "[epoch: 2, 8000] running_loss: 2.049\n",
      "[epoch: 2, 10000] running_loss: 2.029\n",
      "Epoch 2, Train Loss: 0.513, Val Loss: 0.510, Val Acc: 26.280%\n",
      "[epoch: 3, 2000] running_loss: 2.019\n",
      "[epoch: 3, 4000] running_loss: 1.998\n",
      "[epoch: 3, 6000] running_loss: 2.018\n",
      "[epoch: 3, 8000] running_loss: 2.000\n",
      "[epoch: 3, 10000] running_loss: 2.002\n",
      "Epoch 3, Train Loss: 0.502, Val Loss: 0.502, Val Acc: 27.630%\n",
      "[epoch: 4, 2000] running_loss: 1.992\n",
      "[epoch: 4, 4000] running_loss: 1.982\n",
      "[epoch: 4, 6000] running_loss: 1.986\n",
      "[epoch: 4, 8000] running_loss: 1.968\n",
      "[epoch: 4, 10000] running_loss: 1.980\n",
      "Epoch 4, Train Loss: 0.495, Val Loss: 0.496, Val Acc: 29.310%\n",
      "[epoch: 5, 2000] running_loss: 1.983\n",
      "[epoch: 5, 4000] running_loss: 1.961\n",
      "[epoch: 5, 6000] running_loss: 1.956\n",
      "[epoch: 5, 8000] running_loss: 1.952\n",
      "[epoch: 5, 10000] running_loss: 1.952\n",
      "Epoch 5, Train Loss: 0.490, Val Loss: 0.493, Val Acc: 29.430%\n",
      "(0.4901717049509287, 0.4934287242770195, 29.43, 30.099999999999998)\n",
      "======= End Experiment ========\n",
      "======= Start Experiment ========\n",
      "Optimizer: Adagrad, learning_rate: 0.01\n",
      "[epoch: 1, 2000] running_loss: 2.173\n",
      "[epoch: 1, 4000] running_loss: 2.080\n",
      "[epoch: 1, 6000] running_loss: 2.053\n",
      "[epoch: 1, 8000] running_loss: 2.014\n",
      "[epoch: 1, 10000] running_loss: 2.008\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [175]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m args\u001b[38;5;241m.\u001b[39mopt \u001b[38;5;241m=\u001b[39m var1\n\u001b[0;32m     11\u001b[0m args\u001b[38;5;241m.\u001b[39mlr \u001b[38;5;241m=\u001b[39m var2\n\u001b[1;32m---> 12\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mexperiments\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m======= End Experiment ========\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[1;32mIn [171]\u001b[0m, in \u001b[0;36mexperiments\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     54\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m     55\u001b[0m inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m32\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m32\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     59\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "list_var1 = ['Adam', 'Adagrad']        # layers\n",
    "list_var2 = [0.001, 0.01]   # hidden_dim\n",
    "\n",
    "\n",
    "for var1 in list_var1:\n",
    "    for var2 in list_var2:\n",
    "        print('======= Start Experiment ========')\n",
    "        print('Optimizer: {}, learning_rate: {}'.format(var1, var2))\n",
    "\n",
    "        args.opt = var1\n",
    "        args.lr = var2\n",
    "        result = experiments(args)\n",
    "        print(result)\n",
    "        print('======= End Experiment ========')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11191c92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
