{"cells":[{"cell_type":"code","execution_count":1,"id":"93993786","metadata":{"id":"93993786","executionInfo":{"status":"ok","timestamp":1655179640494,"user_tz":-540,"elapsed":3392,"user":{"displayName":"","userId":""}}},"outputs":[],"source":["import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.optim as optim\n","import torch.nn as nn\n","\n","import numpy as np\n","import pandas as pd\n","\n","from tqdm.notebook import tqdm\n","\n","import argparse"]},{"cell_type":"markdown","id":"77a106b9","metadata":{"id":"77a106b9"},"source":["# 1. Train_Val Split"]},{"cell_type":"code","execution_count":2,"id":"4355cd81","metadata":{"id":"4355cd81","executionInfo":{"status":"ok","timestamp":1655179652115,"user_tz":-540,"elapsed":11624,"user":{"displayName":"","userId":""}},"outputId":"d04f6b14-cb94-46f0-b2dd-aa6f617b9794","colab":{"base_uri":"https://localhost:8080/","height":121,"referenced_widgets":["79b0e49828f24d35a862f07acfe5234d","619fbcbee8b34a2c9d081693cd50cee5","6cdb920cc0864a27ac6a6257357aef30","e58f1dd1fd3b4988aa71920517cce315","3825bfd0b59443c3bd7aac7b09a42c73","41a33beb3ce142889727af4cfa609422","71d5512686eb4d4d9d38a34bcc4cb66f","c969909865ef4c08add96692fe560f4c","ab48b13cbd6a4b56aa49f54f1efa2a02","d9368458619f491e9e1bacce1acb6606","e743ae625619424c8c713694f5bf7931"]}},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/170498071 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79b0e49828f24d35a862f07acfe5234d"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n","Files already downloaded and verified\n"]}],"source":["transform = transforms.Compose(\n","    [transforms.ToTensor(),    # image파일을 0 ~ 1사이의 값을 갖는 Tensor로 변환(0: 검은색)\n","     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5), )])  # 채널별로 0.5를 빼고(-0.5 ~ 0.5), 0.5로 나눔\n","\n","batch_size = 128\n","\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n","                                        download=True, transform=transform)\n","# train val split\n","trainset, valset = torch.utils.data.random_split(trainset, [40000, 10000])\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                       download=True, transform=transform)\n","\n","classes = ('plane', 'car', 'bird', 'cat',\n","           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"]},{"cell_type":"markdown","id":"c91c803a","metadata":{"id":"c91c803a"},"source":["# 2. Define MLP Model"]},{"cell_type":"code","execution_count":3,"id":"2dcc94f9","metadata":{"id":"2dcc94f9","executionInfo":{"status":"ok","timestamp":1655179652116,"user_tz":-540,"elapsed":18,"user":{"displayName":"","userId":""}}},"outputs":[],"source":["class MLP(nn.Module):\n","    def __init__(self, in_dim, out_dim, hid_dim, n_layer, act, dropout, bn=True):\n","        super(MLP,self).__init__()\n","        self.in_dim = in_dim\n","        self.out_dim = out_dim\n","        self.hid_dim = hid_dim\n","        self.n_layer = n_layer\n","        self.act = act\n","        self.dropout = dropout\n","        self.bn = bn\n","        \n","        self.fc1 = nn.Linear(self.in_dim, self.hid_dim)\n","        self.linears = nn.ModuleList()\n","        self.bns = nn.ModuleList()\n","        for i in range(self.n_layer):    # n_layer: hidden layer의 개수\n","            self.linears.append(nn.Linear(self.hid_dim, self.hid_dim))\n","            if self.bn:\n","                self.bns.append(nn.BatchNorm1d(hid_dim))\n","        self.fc2 = nn.Linear(self.hid_dim, self.out_dim)\n","        \n","        self.do = nn.Dropout(self.dropout)\n","            \n","        if self.act == 'relu':\n","            self.act = nn.ReLU()\n","        elif self.act == 'leakyrelu':\n","            self.act = nn.LeakyReLU()\n","        else:\n","            raise ValueError(\"Invalid activation function!\")\n","            \n","        \n","    def forward(self, x):\n","        x = self.act(self.fc1(x))\n","        for i in range(len(self.linears)):\n","            x = self.act(self.linears[i](x))\n","            if self.bn:\n","                x = self.bns[i](x)\n","            x = self.do(x)\n","        x = self.fc2(x)          # Softmax 마지막 layer는 activation func이 없어야 함\n","        return x"]},{"cell_type":"code","execution_count":4,"id":"19ebcfb2","metadata":{"id":"19ebcfb2","executionInfo":{"status":"ok","timestamp":1655179652116,"user_tz":-540,"elapsed":17,"user":{"displayName":"","userId":""}},"outputId":"153debd4-b947-4d52-e02e-2b743ec46dd3","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["MLP(\n","  (fc1): Linear(in_features=3072, out_features=200, bias=True)\n","  (linears): ModuleList(\n","    (0): Linear(in_features=200, out_features=200, bias=True)\n","  )\n","  (bns): ModuleList(\n","    (0): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  )\n","  (fc2): Linear(in_features=200, out_features=10, bias=True)\n","  (do): Dropout(p=0.1, inplace=False)\n","  (act): ReLU()\n",")\n"]}],"source":["model = MLP(32*32*3, 10, 200, 1, 'relu', 0.1, True)\n","print(model)"]},{"cell_type":"markdown","id":"b9280e3e","metadata":{"id":"b9280e3e"},"source":["# 3. Experiment"]},{"cell_type":"code","execution_count":5,"id":"ca75257d","metadata":{"id":"ca75257d","executionInfo":{"status":"ok","timestamp":1655179652636,"user_tz":-540,"elapsed":534,"user":{"displayName":"","userId":""}}},"outputs":[],"source":["def experiments(args):\n","\n","    # batch-size test\n","    trainloader = torch.utils.data.DataLoader(trainset, batch_size=args.batch_size,\n","                                            shuffle=True, num_workers=2)\n","    valloader = torch.utils.data.DataLoader(valset, batch_size=args.batch_size, \n","                                            shuffle=False, num_workers=2)\n","\n","    testloader = torch.utils.data.DataLoader(testset, batch_size=args.batch_size,\n","                                            shuffle=False, num_workers=2)\n","\n","\n","    # load model\n","    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","    model = MLP(args.in_dim, args.out_dim, args.hid_dim, args.n_layer, args.act, args.dropout, args.bn)\n","    model = model.to(device)\n","\n","    print('model on {}'.format(device))\n","\n","    criterion = nn.CrossEntropyLoss()\n","    \n","    if args.opt == 'SGD':\n","        optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.mm)\n","    elif args.opt == 'Adam':\n","        optimizer = optim.Adam(model.parameters(), lr=args.lr, )\n","    elif args.opt == 'Adagrad':\n","        optimizer = optim.Adagrad(model.parameters(), lr=args.lr)\n","    else:\n","        raise ValueError('Invalid optimizer!')\n","    \n","    train_losses = []\n","    val_losses = []\n","    val_accs = []\n","    test_accs = []\n","    \n","    for epoch in range(1, args.epochs+1):\n","\n","        ## === Train === ##\n","        running_loss = 0.0\n","        tr_loss = 0.0\n","        # for i, data in enumerate(tqdm(trainloader, 0)):\n","        for i, data in enumerate(trainloader, 0):\n","            # get the inputs\n","            inputs, labels = data\n","            inputs = inputs.view(-1, 32*32*3)\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","\n","            # zero the parameter gradients\n","            optimizer.zero_grad()\n","\n","            # forward, backward, optimize\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            # print log\n","            running_loss += loss.item()\n","            tr_loss += loss.item()\n","            if i % 2000 == 1999:    # 2000 mini batches마다 출력\n","                print('[epoch: {}, {}] running_loss: {:.3f}'.format(epoch, i+1, running_loss / 2000))\n","                running_loss = 0.0\n","                \n","        tr_loss = tr_loss / len(trainloader)\n","                \n","        ## === Validation === ##\n","        correct = 0\n","        total = 0\n","        val_loss = 0.0\n","        with torch.no_grad():\n","            for data in valloader:\n","                inputs, labels = data\n","                inputs = inputs.view(-1, 32*32*3)\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                outputs = model(inputs)\n","                loss = criterion(outputs, labels)\n","\n","                val_loss += loss.item()    # batch에 대한 loss\n","                _, preds = torch.max(outputs, 1)\n","                total += labels.size(0)\n","                correct += (preds == labels).sum().item()\n","\n","            val_loss = val_loss / len(valloader)\n","            val_acc = correct / total * 100\n","       \n","        print('Epoch {}, Train Loss: {:.5f}, Val Loss: {:.5f}, Val Acc: {:.3f}%'.format(\n","            epoch,tr_loss, val_loss, val_acc))\n","        \n","        train_losses.append(tr_loss)\n","        val_losses.append(val_loss)\n","        val_accs.append(val_acc)\n","                    \n","    # ## === Test set === ##\n","    # correct = 0\n","    # total = 0\n","\n","    # with torch.no_grad():\n","    #     for data in testloader:\n","    #         inputs, labels = data\n","    #         inputs = inputs.view(-1, 32*32*3)\n","    #         inputs = inputs.to(device)\n","    #         labels = labels.to(device)\n","\n","    #         outputs = model(inputs)\n","    #         _, preds = torch.max(outputs, 1)\n","    #         total += labels.size(0)\n","    #         correct += (preds == labels).sum().item()\n","    #     test_acc = correct / total * 100\n","\n","\n","    result = {}\n","    result['train_losses'] = train_losses\n","    result['val_losses'] = val_losses\n","    result['val_accs'] = val_accs\n","        \n","    return vars(args), result"]},{"cell_type":"code","source":["import json\n","\n","def save_res(result, params, name_var1, name_var2, var1, var2):\n","    with open(f\"/content/drive/MyDrive/SADL/results/{name_var1}{var1}-{name_var2}{var2}.json\", \"w\") as f:\n","        json.dump(result, f)\n","    with open(f\"/content/drive/MyDrive/SADL/results/{name_var1}{var1}-{name_var2}{var2}_args.json\", \"w\") as f:\n","        json.dump(params, f)\n","    print('save result!')"],"metadata":{"id":"wM7vF1t2x-Gi","executionInfo":{"status":"ok","timestamp":1655180233170,"user_tz":-540,"elapsed":446,"user":{"displayName":"","userId":""}}},"id":"wM7vF1t2x-Gi","execution_count":14,"outputs":[]},{"cell_type":"code","execution_count":15,"id":"1a165e8d","metadata":{"id":"1a165e8d","executionInfo":{"status":"ok","timestamp":1655180233594,"user_tz":-540,"elapsed":4,"user":{"displayName":"","userId":""}}},"outputs":[],"source":["seed= 1212\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","\n","parser = argparse.ArgumentParser()\n","args = parser.parse_args(\"\")\n","\n","args.batch_size = 512\n","\n","args.in_dim = 3 * 32 * 32\n","args.out_dim = 10\n","args.hid_dim = 100\n","args.n_layer = 2\n","args.act = 'relu'\n","args.dropout = 0.1\n","args.bn = True\n","\n","args.opt = 'Adam'\n","\n","args.lr = 0.01\n","args.mm = 0.9\n","\n","args.epochs = 10"]},{"cell_type":"code","execution_count":16,"id":"63d94b02","metadata":{"id":"63d94b02","executionInfo":{"status":"ok","timestamp":1655181317510,"user_tz":-540,"elapsed":1083624,"user":{"displayName":"","userId":""}},"outputId":"92769e97-da07-4634-b6e1-4782a173e059","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["======= Start Experiment ========\n","n_layer: 2, hid_dim: 50\n","model on cuda\n","Epoch 1, Train Loss: 1.86944, Val Loss: 1.72633, Val Acc: 37.770%\n","Epoch 2, Train Loss: 1.67797, Val Loss: 1.64338, Val Acc: 40.730%\n","Epoch 3, Train Loss: 1.58345, Val Loss: 1.58548, Val Acc: 43.540%\n","Epoch 4, Train Loss: 1.51561, Val Loss: 1.54079, Val Acc: 45.240%\n","Epoch 5, Train Loss: 1.46444, Val Loss: 1.52683, Val Acc: 45.330%\n","Epoch 6, Train Loss: 1.42901, Val Loss: 1.47997, Val Acc: 47.140%\n","Epoch 7, Train Loss: 1.38958, Val Loss: 1.48482, Val Acc: 47.770%\n","Epoch 8, Train Loss: 1.36011, Val Loss: 1.48842, Val Acc: 47.100%\n","Epoch 9, Train Loss: 1.33611, Val Loss: 1.46215, Val Acc: 48.030%\n","Epoch 10, Train Loss: 1.30908, Val Loss: 1.43643, Val Acc: 49.750%\n","save result!\n","{'train_losses': [1.8694393529167659, 1.6779745183413541, 1.5834493606905393, 1.5156116621403755, 1.4644438963902147, 1.4290100562421582, 1.3895791904835761, 1.3601095570793635, 1.3361099566085428, 1.3090776084344597], 'val_losses': [1.726333999633789, 1.6433834910392762, 1.5854796230793, 1.5407932639122008, 1.5268347680568695, 1.479972815513611, 1.484818685054779, 1.4884249866008759, 1.4621500968933105, 1.43642618060112], 'val_accs': [37.769999999999996, 40.73, 43.54, 45.24, 45.33, 47.14, 47.77, 47.099999999999994, 48.03, 49.75]}\n","======== End Experiment ========\n","======= Start Experiment ========\n","n_layer: 2, hid_dim: 100\n","model on cuda\n","Epoch 1, Train Loss: 1.84065, Val Loss: 1.71331, Val Acc: 37.820%\n","Epoch 2, Train Loss: 1.63486, Val Loss: 1.58033, Val Acc: 43.560%\n","Epoch 3, Train Loss: 1.51906, Val Loss: 1.51450, Val Acc: 45.570%\n","Epoch 4, Train Loss: 1.44157, Val Loss: 1.49194, Val Acc: 46.800%\n","Epoch 5, Train Loss: 1.38703, Val Loss: 1.44532, Val Acc: 48.830%\n","Epoch 6, Train Loss: 1.33619, Val Loss: 1.44437, Val Acc: 48.780%\n","Epoch 7, Train Loss: 1.28905, Val Loss: 1.42030, Val Acc: 49.980%\n","Epoch 8, Train Loss: 1.24874, Val Loss: 1.42058, Val Acc: 50.000%\n","Epoch 9, Train Loss: 1.21722, Val Loss: 1.41897, Val Acc: 50.150%\n","Epoch 10, Train Loss: 1.18411, Val Loss: 1.41593, Val Acc: 50.470%\n","save result!\n","{'train_losses': [1.8406515362896496, 1.6348618389684944, 1.519061370740963, 1.441572895532922, 1.387027595616594, 1.3361906703514388, 1.2890474600127981, 1.2487410880342316, 1.2172164117233664, 1.1841105901742284], 'val_losses': [1.7133105456829072, 1.5803269624710083, 1.5145048797130585, 1.4919416546821593, 1.4453249037265778, 1.4443670213222504, 1.4203039526939392, 1.4205789804458617, 1.4189725875854493, 1.4159346342086792], 'val_accs': [37.82, 43.56, 45.57, 46.800000000000004, 48.83, 48.78, 49.980000000000004, 50.0, 50.14999999999999, 50.470000000000006]}\n","======== End Experiment ========\n","======= Start Experiment ========\n","n_layer: 2, hid_dim: 150\n","model on cuda\n","Epoch 1, Train Loss: 1.84880, Val Loss: 1.72349, Val Acc: 36.910%\n","Epoch 2, Train Loss: 1.63380, Val Loss: 1.60638, Val Acc: 42.520%\n","Epoch 3, Train Loss: 1.51071, Val Loss: 1.52015, Val Acc: 45.340%\n","Epoch 4, Train Loss: 1.42661, Val Loss: 1.45385, Val Acc: 47.550%\n","Epoch 5, Train Loss: 1.34989, Val Loss: 1.42470, Val Acc: 48.670%\n","Epoch 6, Train Loss: 1.29667, Val Loss: 1.39624, Val Acc: 50.400%\n","Epoch 7, Train Loss: 1.24493, Val Loss: 1.43151, Val Acc: 49.130%\n","Epoch 8, Train Loss: 1.19522, Val Loss: 1.38812, Val Acc: 51.120%\n","Epoch 9, Train Loss: 1.16591, Val Loss: 1.37883, Val Acc: 51.640%\n","Epoch 10, Train Loss: 1.11937, Val Loss: 1.42059, Val Acc: 50.690%\n","save result!\n","{'train_losses': [1.8487996134576918, 1.6338011614884003, 1.510713595378248, 1.4266108757332912, 1.349891709375985, 1.29666790630244, 1.2449318804318392, 1.1952219401733786, 1.1659140933918049, 1.1193720803985112], 'val_losses': [1.723487651348114, 1.6063775897026062, 1.5201456189155578, 1.4538456797599792, 1.4247045814990997, 1.3962363064289094, 1.4315119862556458, 1.388115608692169, 1.3788265109062194, 1.4205869376659392], 'val_accs': [36.91, 42.52, 45.34, 47.55, 48.67, 50.4, 49.13, 51.12, 51.64, 50.690000000000005]}\n","======== End Experiment ========\n","======= Start Experiment ========\n","n_layer: 2, hid_dim: 200\n","model on cuda\n","Epoch 1, Train Loss: 1.87159, Val Loss: 1.75076, Val Acc: 36.690%\n","Epoch 2, Train Loss: 1.64209, Val Loss: 1.58903, Val Acc: 42.750%\n","Epoch 3, Train Loss: 1.50190, Val Loss: 1.50743, Val Acc: 46.180%\n","Epoch 4, Train Loss: 1.41314, Val Loss: 1.46087, Val Acc: 47.210%\n","Epoch 5, Train Loss: 1.34196, Val Loss: 1.41982, Val Acc: 49.060%\n","Epoch 6, Train Loss: 1.27642, Val Loss: 1.41999, Val Acc: 49.800%\n","Epoch 7, Train Loss: 1.23511, Val Loss: 1.40474, Val Acc: 50.680%\n","Epoch 8, Train Loss: 1.18100, Val Loss: 1.40065, Val Acc: 50.430%\n","Epoch 9, Train Loss: 1.13198, Val Loss: 1.40150, Val Acc: 51.690%\n","Epoch 10, Train Loss: 1.09240, Val Loss: 1.41825, Val Acc: 51.070%\n","save result!\n","{'train_losses': [1.87159231191949, 1.6420898844924154, 1.5019036591807498, 1.413144108615344, 1.3419616720344447, 1.276418759853025, 1.2351078821133963, 1.181002494655078, 1.131977215597901, 1.092400792278821], 'val_losses': [1.7507557570934296, 1.5890330493450164, 1.5074333608150483, 1.460867840051651, 1.419818264245987, 1.4199935436248778, 1.4047388672828673, 1.4006512582302093, 1.4015019059181213, 1.4182461619377136], 'val_accs': [36.69, 42.75, 46.18, 47.21, 49.059999999999995, 49.8, 50.68, 50.43, 51.690000000000005, 51.07000000000001]}\n","======== End Experiment ========\n","======= Start Experiment ========\n","n_layer: 3, hid_dim: 50\n","model on cuda\n","Epoch 1, Train Loss: 1.88394, Val Loss: 1.76519, Val Acc: 36.470%\n","Epoch 2, Train Loss: 1.70727, Val Loss: 1.67820, Val Acc: 39.480%\n","Epoch 3, Train Loss: 1.61812, Val Loss: 1.60081, Val Acc: 42.540%\n","Epoch 4, Train Loss: 1.55034, Val Loss: 1.55815, Val Acc: 44.050%\n","Epoch 5, Train Loss: 1.50102, Val Loss: 1.52370, Val Acc: 45.590%\n","Epoch 6, Train Loss: 1.45518, Val Loss: 1.50330, Val Acc: 46.380%\n","Epoch 7, Train Loss: 1.41707, Val Loss: 1.51031, Val Acc: 46.590%\n","Epoch 8, Train Loss: 1.39684, Val Loss: 1.49508, Val Acc: 47.710%\n","Epoch 9, Train Loss: 1.36496, Val Loss: 1.48672, Val Acc: 46.980%\n","Epoch 10, Train Loss: 1.33518, Val Loss: 1.49407, Val Acc: 47.540%\n","save result!\n","{'train_losses': [1.8839370133001594, 1.7072745848305617, 1.61811689938171, 1.5503421312646022, 1.5010165413723717, 1.455181355717816, 1.417065831679332, 1.396839816359025, 1.3649568452110774, 1.3351778984069824], 'val_losses': [1.7651910066604615, 1.6782033264636993, 1.6008136034011842, 1.5581486403942109, 1.5237041115760803, 1.5032999157905578, 1.5103084921836853, 1.4950775682926178, 1.48671515583992, 1.4940722107887268], 'val_accs': [36.47, 39.48, 42.54, 44.05, 45.59, 46.379999999999995, 46.589999999999996, 47.71, 46.98, 47.54]}\n","======== End Experiment ========\n","======= Start Experiment ========\n","n_layer: 3, hid_dim: 100\n","model on cuda\n","Epoch 1, Train Loss: 1.85933, Val Loss: 1.73318, Val Acc: 37.030%\n","Epoch 2, Train Loss: 1.64832, Val Loss: 1.61862, Val Acc: 42.410%\n","Epoch 3, Train Loss: 1.54395, Val Loss: 1.52945, Val Acc: 46.110%\n","Epoch 4, Train Loss: 1.46283, Val Loss: 1.49241, Val Acc: 47.150%\n","Epoch 5, Train Loss: 1.39185, Val Loss: 1.48569, Val Acc: 47.060%\n","Epoch 6, Train Loss: 1.34733, Val Loss: 1.43640, Val Acc: 49.040%\n","Epoch 7, Train Loss: 1.30139, Val Loss: 1.42328, Val Acc: 49.560%\n","Epoch 8, Train Loss: 1.26930, Val Loss: 1.42173, Val Acc: 49.850%\n","Epoch 9, Train Loss: 1.23880, Val Loss: 1.42571, Val Acc: 49.600%\n","Epoch 10, Train Loss: 1.20132, Val Loss: 1.41918, Val Acc: 50.320%\n","save result!\n","{'train_losses': [1.859334167045883, 1.6483208257940751, 1.5439475065545192, 1.4628344789335999, 1.3918522656718386, 1.3473286613633362, 1.3013896429086034, 1.2693022127392926, 1.238799689691278, 1.2013196145431906], 'val_losses': [1.7331805646419525, 1.6186241626739502, 1.529445093870163, 1.4924125015735625, 1.4856920301914216, 1.4364009201526642, 1.4232763409614564, 1.42172868847847, 1.4257076680660248, 1.4191809475421906], 'val_accs': [37.03, 42.41, 46.11, 47.15, 47.06, 49.04, 49.559999999999995, 49.85, 49.6, 50.32]}\n","======== End Experiment ========\n","======= Start Experiment ========\n","n_layer: 3, hid_dim: 150\n","model on cuda\n","Epoch 1, Train Loss: 1.87475, Val Loss: 1.74875, Val Acc: 36.020%\n","Epoch 2, Train Loss: 1.66580, Val Loss: 1.60930, Val Acc: 42.100%\n","Epoch 3, Train Loss: 1.54959, Val Loss: 1.53516, Val Acc: 45.240%\n","Epoch 4, Train Loss: 1.46013, Val Loss: 1.48237, Val Acc: 47.480%\n","Epoch 5, Train Loss: 1.39481, Val Loss: 1.45285, Val Acc: 48.400%\n","Epoch 6, Train Loss: 1.34856, Val Loss: 1.42238, Val Acc: 49.080%\n","Epoch 7, Train Loss: 1.29242, Val Loss: 1.41160, Val Acc: 49.820%\n","Epoch 8, Train Loss: 1.23413, Val Loss: 1.40891, Val Acc: 50.390%\n","Epoch 9, Train Loss: 1.20917, Val Loss: 1.40138, Val Acc: 50.730%\n","Epoch 10, Train Loss: 1.16771, Val Loss: 1.41181, Val Acc: 50.550%\n","save result!\n","{'train_losses': [1.8747531975371927, 1.6658022931859464, 1.549593214747272, 1.4601268602322928, 1.394808734519572, 1.3485575522048563, 1.2924200326581545, 1.2341295450548582, 1.209169319913357, 1.1677068939691857], 'val_losses': [1.7487509429454804, 1.6093034267425537, 1.5351615786552428, 1.4823733866214752, 1.4528525710105895, 1.422377109527588, 1.4115952789783477, 1.4089094758033753, 1.4013752937316895, 1.411812287569046], 'val_accs': [36.02, 42.1, 45.24, 47.48, 48.4, 49.08, 49.82, 50.39, 50.73, 50.55]}\n","======== End Experiment ========\n","======= Start Experiment ========\n","n_layer: 3, hid_dim: 200\n","model on cuda\n","Epoch 1, Train Loss: 1.87493, Val Loss: 1.73489, Val Acc: 37.060%\n","Epoch 2, Train Loss: 1.65305, Val Loss: 1.62369, Val Acc: 42.220%\n","Epoch 3, Train Loss: 1.54257, Val Loss: 1.52797, Val Acc: 45.160%\n","Epoch 4, Train Loss: 1.44040, Val Loss: 1.47052, Val Acc: 47.410%\n","Epoch 5, Train Loss: 1.36927, Val Loss: 1.45298, Val Acc: 48.880%\n","Epoch 6, Train Loss: 1.30958, Val Loss: 1.41577, Val Acc: 49.900%\n","Epoch 7, Train Loss: 1.25164, Val Loss: 1.39741, Val Acc: 50.790%\n","Epoch 8, Train Loss: 1.20640, Val Loss: 1.41096, Val Acc: 49.630%\n","Epoch 9, Train Loss: 1.16773, Val Loss: 1.39156, Val Acc: 51.200%\n","Epoch 10, Train Loss: 1.12392, Val Loss: 1.39379, Val Acc: 50.840%\n","save result!\n","{'train_losses': [1.8749340153947662, 1.6530526604833482, 1.5425692162936246, 1.4404012145875376, 1.3692683901967881, 1.3095780067806002, 1.2516418393654158, 1.2063953363442723, 1.1677285432815552, 1.1239228354224675], 'val_losses': [1.7348867893218993, 1.6236895501613617, 1.52797269821167, 1.470523375272751, 1.4529817342758178, 1.4157675981521607, 1.397406280040741, 1.4109627187252045, 1.3915644109249115, 1.3937916040420533], 'val_accs': [37.059999999999995, 42.22, 45.16, 47.410000000000004, 48.88, 49.9, 50.79, 49.63, 51.2, 50.839999999999996]}\n","======== End Experiment ========\n","======= Start Experiment ========\n","n_layer: 4, hid_dim: 50\n","model on cuda\n","Epoch 1, Train Loss: 1.92069, Val Loss: 1.78838, Val Acc: 35.960%\n","Epoch 2, Train Loss: 1.73406, Val Loss: 1.69143, Val Acc: 39.000%\n","Epoch 3, Train Loss: 1.63538, Val Loss: 1.63183, Val Acc: 42.430%\n","Epoch 4, Train Loss: 1.57854, Val Loss: 1.58487, Val Acc: 43.050%\n","Epoch 5, Train Loss: 1.52684, Val Loss: 1.57003, Val Acc: 44.180%\n","Epoch 6, Train Loss: 1.48815, Val Loss: 1.54466, Val Acc: 45.330%\n","Epoch 7, Train Loss: 1.44783, Val Loss: 1.52798, Val Acc: 45.910%\n","Epoch 8, Train Loss: 1.41753, Val Loss: 1.50735, Val Acc: 46.360%\n","Epoch 9, Train Loss: 1.39439, Val Loss: 1.50960, Val Acc: 46.920%\n","Epoch 10, Train Loss: 1.37876, Val Loss: 1.49095, Val Acc: 47.270%\n","save result!\n","{'train_losses': [1.9206850196741805, 1.7340558329715003, 1.6353782852993737, 1.5785442892509172, 1.5268417808073986, 1.488149897961677, 1.4478263025042377, 1.4175262436082092, 1.3943867457063892, 1.378761247743534], 'val_losses': [1.788382363319397, 1.6914292812347411, 1.6318302869796752, 1.5848658502101898, 1.5700271725654602, 1.544664841890335, 1.5279837787151336, 1.5073459565639495, 1.5096033990383149, 1.4909464776515962], 'val_accs': [35.96, 39.0, 42.43, 43.05, 44.18, 45.33, 45.910000000000004, 46.36, 46.92, 47.27]}\n","======== End Experiment ========\n","======= Start Experiment ========\n","n_layer: 4, hid_dim: 100\n","model on cuda\n","Epoch 1, Train Loss: 1.87505, Val Loss: 1.75695, Val Acc: 36.660%\n","Epoch 2, Train Loss: 1.68177, Val Loss: 1.65174, Val Acc: 40.830%\n","Epoch 3, Train Loss: 1.59195, Val Loss: 1.59421, Val Acc: 42.590%\n","Epoch 4, Train Loss: 1.50768, Val Loss: 1.52076, Val Acc: 45.530%\n","Epoch 5, Train Loss: 1.44342, Val Loss: 1.49416, Val Acc: 47.170%\n","Epoch 6, Train Loss: 1.39868, Val Loss: 1.46491, Val Acc: 47.970%\n","Epoch 7, Train Loss: 1.34598, Val Loss: 1.44686, Val Acc: 48.590%\n","Epoch 8, Train Loss: 1.30658, Val Loss: 1.45898, Val Acc: 48.520%\n","Epoch 9, Train Loss: 1.26787, Val Loss: 1.43185, Val Acc: 49.510%\n","Epoch 10, Train Loss: 1.23954, Val Loss: 1.43460, Val Acc: 49.700%\n","save result!\n","{'train_losses': [1.875046959406213, 1.681768337382546, 1.5919481591333318, 1.5076761004291004, 1.4434201958813244, 1.398684807970554, 1.3459829937053631, 1.3065781427335135, 1.267866178403927, 1.2395359307904787], 'val_losses': [1.7569500923156738, 1.6517419278621674, 1.5942133724689485, 1.520763885974884, 1.4941592991352082, 1.4649091422557832, 1.4468609988689423, 1.4589797914028169, 1.4318468630313874, 1.434601616859436], 'val_accs': [36.66, 40.83, 42.59, 45.53, 47.17, 47.97, 48.59, 48.52, 49.51, 49.7]}\n","======== End Experiment ========\n","======= Start Experiment ========\n","n_layer: 4, hid_dim: 150\n","model on cuda\n","Epoch 1, Train Loss: 1.88720, Val Loss: 1.78823, Val Acc: 34.470%\n","Epoch 2, Train Loss: 1.68256, Val Loss: 1.65159, Val Acc: 40.200%\n","Epoch 3, Train Loss: 1.57269, Val Loss: 1.55185, Val Acc: 44.180%\n","Epoch 4, Train Loss: 1.48072, Val Loss: 1.50334, Val Acc: 46.020%\n","Epoch 5, Train Loss: 1.40729, Val Loss: 1.46490, Val Acc: 48.230%\n","Epoch 6, Train Loss: 1.35060, Val Loss: 1.44943, Val Acc: 48.760%\n","Epoch 7, Train Loss: 1.30303, Val Loss: 1.43558, Val Acc: 48.870%\n","Epoch 8, Train Loss: 1.25334, Val Loss: 1.44951, Val Acc: 49.000%\n","Epoch 9, Train Loss: 1.22184, Val Loss: 1.41459, Val Acc: 50.140%\n","Epoch 10, Train Loss: 1.18011, Val Loss: 1.43952, Val Acc: 50.060%\n","save result!\n","{'train_losses': [1.8871965423414978, 1.6825569068329245, 1.572691663911071, 1.480717269680168, 1.4072890221318113, 1.3506028501293328, 1.303031735782382, 1.2533432924294774, 1.2218380381789389, 1.1801069597654705], 'val_losses': [1.7882345557212829, 1.6515863537788391, 1.551854521036148, 1.50334193110466, 1.4648968040943147, 1.4494340360164641, 1.4355838894844055, 1.4495141088962555, 1.41459179520607, 1.4395183086395265], 'val_accs': [34.47, 40.2, 44.18, 46.02, 48.230000000000004, 48.76, 48.870000000000005, 49.0, 50.13999999999999, 50.06]}\n","======== End Experiment ========\n","======= Start Experiment ========\n","n_layer: 4, hid_dim: 200\n","model on cuda\n","Epoch 1, Train Loss: 1.87742, Val Loss: 1.74544, Val Acc: 37.120%\n","Epoch 2, Train Loss: 1.65504, Val Loss: 1.63716, Val Acc: 41.410%\n","Epoch 3, Train Loss: 1.55715, Val Loss: 1.55422, Val Acc: 43.680%\n","Epoch 4, Train Loss: 1.45658, Val Loss: 1.51777, Val Acc: 45.850%\n","Epoch 5, Train Loss: 1.38976, Val Loss: 1.47084, Val Acc: 47.110%\n","Epoch 6, Train Loss: 1.33858, Val Loss: 1.45197, Val Acc: 48.620%\n","Epoch 7, Train Loss: 1.28282, Val Loss: 1.40243, Val Acc: 50.550%\n","Epoch 8, Train Loss: 1.23557, Val Loss: 1.41620, Val Acc: 50.480%\n","Epoch 9, Train Loss: 1.19577, Val Loss: 1.41391, Val Acc: 50.740%\n","Epoch 10, Train Loss: 1.14859, Val Loss: 1.39607, Val Acc: 51.310%\n","save result!\n","{'train_losses': [1.8774244845667971, 1.6550433243377298, 1.5571481774124918, 1.4565824783301051, 1.3897627637356142, 1.3385784384570545, 1.2828213564957245, 1.235567894163011, 1.1957739440700677, 1.148594328119785], 'val_losses': [1.7454435288906098, 1.6371572613716125, 1.5542194843292236, 1.5177683353424072, 1.4708351969718934, 1.4519717514514923, 1.4024312674999238, 1.4161952316761017, 1.4139066457748413, 1.3960693240165711], 'val_accs': [37.12, 41.410000000000004, 43.68, 45.85, 47.11, 48.620000000000005, 50.55, 50.480000000000004, 50.739999999999995, 51.31]}\n","======== End Experiment ========\n"]}],"source":["list_var1 = [2,3,4]        # layers\n","list_var2 = [50,100,150,200]   # hidden_dim\n","name_var1, name_var2 = 'n_layer', 'hid_dim'\n","\n","res = pd.DataFrame(columns=['train_losses', 'val_losses','val_accs'])\n","for var1 in list_var1:\n","    for var2 in list_var2:\n","        print('======= Start Experiment ========')\n","        print('{}: {}, {}: {}'.format(name_var1, var1, name_var2, var2))\n","\n","        args.n_layer = var1\n","        args.hid_dim = var2\n","        params, result = experiments(args)\n","        \n","        save_res(result, params, name_var1, name_var2, var1, var2)\n","        print(result)\n","        print('======== End Experiment ========')"]},{"cell_type":"markdown","source":["layers: 4, hidden_dim: 200"],"metadata":{"id":"1zdLoTLl8cRH"},"id":"1zdLoTLl8cRH"},{"cell_type":"code","source":["seed= 1212\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","\n","parser = argparse.ArgumentParser()\n","args = parser.parse_args(\"\")\n","\n","args.batch_size = 512\n","\n","args.in_dim = 3 * 32 * 32\n","args.out_dim = 10\n","args.hid_dim = 200\n","args.n_layer = 4\n","args.act = 'relu'\n","args.dropout = 0.1\n","args.bn = True\n","\n","args.opt = 'Adam'\n","\n","args.lr = 0.01\n","args.mm = 0.9\n","\n","args.epochs = 10\n","\n","\n","list_var1 = ['Adam', 'SGD', 'Adagrad']        # optimizer\n","list_var2 = [0.1, 0.01, 0.001]   # learning rate\n","name_var1, name_var2 = 'optim', 'lr'\n","\n","res = pd.DataFrame(columns=['train_losses', 'val_losses','val_accs'])\n","for var1 in list_var1:\n","    for var2 in list_var2:\n","        print('======= Start Experiment ========')\n","        print('{}: {}, {}: {}'.format(name_var1, var1, name_var2, var2))\n","\n","        args.optim = var1\n","        args.lr = var2\n","        params, result = experiments(args)\n","        \n","        save_res(result, params, name_var1, name_var2, var1, var2)\n","        print(result)\n","        print('======== End Experiment ========')"],"metadata":{"id":"qBF_ZhK86m1w","executionInfo":{"status":"ok","timestamp":1655182151152,"user_tz":-540,"elapsed":833672,"user":{"displayName":"","userId":""}},"outputId":"8d83eddf-3b5e-4c25-8ab1-ca79de3ea64a","colab":{"base_uri":"https://localhost:8080/"}},"id":"qBF_ZhK86m1w","execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["======= Start Experiment ========\n","optim: Adam, lr: 0.1\n","model on cuda\n","Epoch 1, Train Loss: 2.38841, Val Loss: 1.96370, Val Acc: 27.330%\n","Epoch 2, Train Loss: 1.89921, Val Loss: 1.85540, Val Acc: 31.570%\n","Epoch 3, Train Loss: 1.81939, Val Loss: 1.87402, Val Acc: 30.830%\n","Epoch 4, Train Loss: 1.78504, Val Loss: 1.76136, Val Acc: 37.480%\n","Epoch 5, Train Loss: 1.72502, Val Loss: 1.70497, Val Acc: 38.880%\n","Epoch 6, Train Loss: 1.66877, Val Loss: 1.68814, Val Acc: 39.950%\n","Epoch 7, Train Loss: 1.65633, Val Loss: 1.67389, Val Acc: 40.020%\n","Epoch 8, Train Loss: 1.61368, Val Loss: 1.65849, Val Acc: 40.180%\n","Epoch 9, Train Loss: 1.58199, Val Loss: 1.62898, Val Acc: 41.080%\n","Epoch 10, Train Loss: 1.54641, Val Loss: 1.61558, Val Acc: 43.410%\n","save result!\n","{'train_losses': [2.388409409341933, 1.8992145619814909, 1.8193931232524823, 1.7850437948975382, 1.7250170466266102, 1.6687698107731492, 1.6563323298587074, 1.6136799311336083, 1.5819877039028118, 1.5464137339893775], 'val_losses': [1.9636986553668976, 1.8553960621356964, 1.8740205705165862, 1.7613595068454742, 1.7049732029438018, 1.6881414294242858, 1.673893892765045, 1.6584860026836394, 1.6289819121360778, 1.615583884716034], 'val_accs': [27.33, 31.569999999999997, 30.830000000000002, 37.480000000000004, 38.879999999999995, 39.95, 40.02, 40.18, 41.08, 43.41]}\n","======== End Experiment ========\n","======= Start Experiment ========\n","optim: Adam, lr: 0.01\n","model on cuda\n","Epoch 1, Train Loss: 1.88426, Val Loss: 1.78409, Val Acc: 35.980%\n","Epoch 2, Train Loss: 1.67809, Val Loss: 1.63053, Val Acc: 41.260%\n","Epoch 3, Train Loss: 1.56891, Val Loss: 1.55778, Val Acc: 44.150%\n","Epoch 4, Train Loss: 1.47222, Val Loss: 1.49610, Val Acc: 46.360%\n","Epoch 5, Train Loss: 1.39944, Val Loss: 1.44636, Val Acc: 48.230%\n","Epoch 6, Train Loss: 1.33331, Val Loss: 1.44710, Val Acc: 48.780%\n","Epoch 7, Train Loss: 1.28239, Val Loss: 1.42407, Val Acc: 49.530%\n","Epoch 8, Train Loss: 1.23797, Val Loss: 1.42166, Val Acc: 49.920%\n","Epoch 9, Train Loss: 1.19883, Val Loss: 1.39535, Val Acc: 50.630%\n","Epoch 10, Train Loss: 1.14888, Val Loss: 1.41606, Val Acc: 51.290%\n","save result!\n","{'train_losses': [1.8842645913739748, 1.6780853784537013, 1.568906385687333, 1.4722234493569484, 1.3994355458247512, 1.333308215382733, 1.282388367230379, 1.2379724707784532, 1.1988320576993725, 1.1488795355905461], 'val_losses': [1.7840861976146698, 1.6305276930332184, 1.5577810466289521, 1.4961040019989014, 1.4463633835315703, 1.4470977365970612, 1.4240654170513154, 1.4216625034809112, 1.395351666212082, 1.4160598456859588], 'val_accs': [35.980000000000004, 41.260000000000005, 44.15, 46.36, 48.230000000000004, 48.78, 49.53, 49.919999999999995, 50.629999999999995, 51.29]}\n","======== End Experiment ========\n","======= Start Experiment ========\n","optim: Adam, lr: 0.001\n","model on cuda\n","Epoch 1, Train Loss: 1.79410, Val Loss: 1.64756, Val Acc: 41.090%\n","Epoch 2, Train Loss: 1.55367, Val Loss: 1.52830, Val Acc: 45.120%\n","Epoch 3, Train Loss: 1.44404, Val Loss: 1.48390, Val Acc: 47.370%\n","Epoch 4, Train Loss: 1.35341, Val Loss: 1.44871, Val Acc: 49.020%\n","Epoch 5, Train Loss: 1.29738, Val Loss: 1.41306, Val Acc: 50.290%\n","Epoch 6, Train Loss: 1.23713, Val Loss: 1.41628, Val Acc: 50.140%\n","Epoch 7, Train Loss: 1.17786, Val Loss: 1.41234, Val Acc: 50.430%\n","Epoch 8, Train Loss: 1.12892, Val Loss: 1.41354, Val Acc: 50.900%\n","Epoch 9, Train Loss: 1.08242, Val Loss: 1.42165, Val Acc: 51.550%\n","Epoch 10, Train Loss: 1.04299, Val Loss: 1.41748, Val Acc: 51.930%\n","save result!\n","{'train_losses': [1.7941034202334247, 1.5536725083483924, 1.4440398623671713, 1.3534080786041067, 1.2973812169666532, 1.237127427813373, 1.177858920036992, 1.1289241223395625, 1.0824165751662436, 1.0429935938195338], 'val_losses': [1.6475584030151367, 1.5283047676086425, 1.4839008808135987, 1.4487148642539978, 1.4130623638629913, 1.4162760078907013, 1.412340271472931, 1.4135374903678894, 1.421653151512146, 1.4174824416637422], 'val_accs': [41.089999999999996, 45.12, 47.370000000000005, 49.02, 50.29, 50.13999999999999, 50.43, 50.9, 51.55, 51.93]}\n","======== End Experiment ========\n","======= Start Experiment ========\n","optim: SGD, lr: 0.1\n","model on cuda\n","Epoch 1, Train Loss: 2.41200, Val Loss: 1.98927, Val Acc: 26.040%\n","Epoch 2, Train Loss: 1.89247, Val Loss: 1.84010, Val Acc: 32.180%\n","Epoch 3, Train Loss: 1.80027, Val Loss: 1.78430, Val Acc: 35.480%\n","Epoch 4, Train Loss: 1.73217, Val Loss: 1.74620, Val Acc: 36.760%\n","Epoch 5, Train Loss: 1.68978, Val Loss: 1.71592, Val Acc: 38.540%\n","Epoch 6, Train Loss: 1.64899, Val Loss: 1.67679, Val Acc: 39.210%\n","Epoch 7, Train Loss: 1.59192, Val Loss: 1.62532, Val Acc: 41.310%\n","Epoch 8, Train Loss: 1.56021, Val Loss: 1.62033, Val Acc: 40.800%\n","Epoch 9, Train Loss: 1.51270, Val Loss: 1.59385, Val Acc: 43.160%\n","Epoch 10, Train Loss: 1.46985, Val Loss: 1.55685, Val Acc: 44.830%\n","save result!\n","{'train_losses': [2.411999999722348, 1.8924713164945193, 1.8002696173100532, 1.7321713589414764, 1.6897766288322738, 1.6489940926998476, 1.5919217550301854, 1.560211320466633, 1.5126963838746277, 1.4698521351512475], 'val_losses': [1.989273089170456, 1.8400993764400482, 1.7842972457408905, 1.7461957454681396, 1.7159215927124023, 1.6767899692058563, 1.6253175914287568, 1.620329350233078, 1.593850302696228, 1.5568466901779174], 'val_accs': [26.040000000000003, 32.18, 35.480000000000004, 36.76, 38.54, 39.21, 41.31, 40.8, 43.16, 44.83]}\n","======== End Experiment ========\n","======= Start Experiment ========\n","optim: SGD, lr: 0.01\n","model on cuda\n","Epoch 1, Train Loss: 1.88078, Val Loss: 1.76297, Val Acc: 35.480%\n","Epoch 2, Train Loss: 1.67560, Val Loss: 1.62328, Val Acc: 41.130%\n","Epoch 3, Train Loss: 1.56490, Val Loss: 1.55392, Val Acc: 44.070%\n","Epoch 4, Train Loss: 1.47045, Val Loss: 1.54613, Val Acc: 44.450%\n","Epoch 5, Train Loss: 1.39736, Val Loss: 1.44562, Val Acc: 48.650%\n","Epoch 6, Train Loss: 1.34035, Val Loss: 1.46686, Val Acc: 48.240%\n","Epoch 7, Train Loss: 1.28283, Val Loss: 1.42177, Val Acc: 49.770%\n","Epoch 8, Train Loss: 1.23835, Val Loss: 1.40401, Val Acc: 50.310%\n","Epoch 9, Train Loss: 1.19188, Val Loss: 1.42022, Val Acc: 50.600%\n","Epoch 10, Train Loss: 1.14970, Val Loss: 1.43181, Val Acc: 50.760%\n","save result!\n","{'train_losses': [1.88078186330916, 1.6756043388873716, 1.564903303037716, 1.470451667338987, 1.3973649272435829, 1.3403502536725393, 1.2828334856636916, 1.2383464786070812, 1.1918827204764644, 1.1496973731849767], 'val_losses': [1.7629655420780181, 1.6232828617095947, 1.553917294740677, 1.5461291432380677, 1.4456178307533265, 1.4668635368347167, 1.4217745304107665, 1.4040144979953766, 1.420220285654068, 1.4318059444427491], 'val_accs': [35.480000000000004, 41.13, 44.07, 44.45, 48.65, 48.24, 49.769999999999996, 50.31, 50.6, 50.760000000000005]}\n","======== End Experiment ========\n","======= Start Experiment ========\n","optim: SGD, lr: 0.001\n","model on cuda\n","Epoch 1, Train Loss: 1.79933, Val Loss: 1.62740, Val Acc: 42.110%\n","Epoch 2, Train Loss: 1.54710, Val Loss: 1.53793, Val Acc: 44.710%\n","Epoch 3, Train Loss: 1.43280, Val Loss: 1.48627, Val Acc: 47.140%\n","Epoch 4, Train Loss: 1.35977, Val Loss: 1.42638, Val Acc: 49.140%\n","Epoch 5, Train Loss: 1.28127, Val Loss: 1.41859, Val Acc: 49.730%\n","Epoch 6, Train Loss: 1.22814, Val Loss: 1.42313, Val Acc: 50.070%\n","Epoch 7, Train Loss: 1.16637, Val Loss: 1.42289, Val Acc: 50.520%\n","Epoch 8, Train Loss: 1.12464, Val Loss: 1.40188, Val Acc: 50.980%\n","Epoch 9, Train Loss: 1.07572, Val Loss: 1.39904, Val Acc: 51.990%\n","Epoch 10, Train Loss: 1.02730, Val Loss: 1.46444, Val Acc: 50.460%\n","save result!\n","{'train_losses': [1.7993321796006794, 1.5471020936965942, 1.4328012587148933, 1.3597740538512604, 1.2812675101847588, 1.2281364383576792, 1.1663664156877542, 1.1246428119985363, 1.0757162261612807, 1.0272991664801971], 'val_losses': [1.627396184206009, 1.5379341840744019, 1.486270785331726, 1.4263755261898041, 1.41858611702919, 1.4231325089931488, 1.4228938221931458, 1.4018756985664367, 1.3990408539772035, 1.4644353091716766], 'val_accs': [42.11, 44.71, 47.14, 49.14, 49.730000000000004, 50.07, 50.519999999999996, 50.980000000000004, 51.99, 50.46000000000001]}\n","======== End Experiment ========\n","======= Start Experiment ========\n","optim: Adagrad, lr: 0.1\n","model on cuda\n","Epoch 1, Train Loss: 2.48152, Val Loss: 2.04258, Val Acc: 24.610%\n","Epoch 2, Train Loss: 1.94315, Val Loss: 1.90467, Val Acc: 29.240%\n","Epoch 3, Train Loss: 1.85187, Val Loss: 1.80665, Val Acc: 34.040%\n","Epoch 4, Train Loss: 1.79240, Val Loss: 1.77338, Val Acc: 35.520%\n","Epoch 5, Train Loss: 1.73593, Val Loss: 1.72337, Val Acc: 37.420%\n","Epoch 6, Train Loss: 1.67735, Val Loss: 1.67193, Val Acc: 39.060%\n","Epoch 7, Train Loss: 1.63299, Val Loss: 1.66460, Val Acc: 39.650%\n","Epoch 8, Train Loss: 1.58872, Val Loss: 1.63665, Val Acc: 41.070%\n","Epoch 9, Train Loss: 1.54852, Val Loss: 1.58169, Val Acc: 42.600%\n","Epoch 10, Train Loss: 1.51705, Val Loss: 1.56418, Val Acc: 44.110%\n","save result!\n","{'train_losses': [2.481516465356078, 1.9431483534317981, 1.851865754851812, 1.792398599129689, 1.7359330488156668, 1.6773529173452644, 1.6329851814463168, 1.5887233033964905, 1.5485239798509622, 1.517049634003941], 'val_losses': [2.042576479911804, 1.9046699464321137, 1.8066450893878936, 1.7733804345130921, 1.7233681440353394, 1.6719348549842834, 1.6645971834659576, 1.6366500973701477, 1.5816938638687135, 1.5641844630241395], 'val_accs': [24.610000000000003, 29.24, 34.04, 35.52, 37.419999999999995, 39.06, 39.65, 41.07, 42.6, 44.11]}\n","======== End Experiment ========\n","======= Start Experiment ========\n","optim: Adagrad, lr: 0.01\n","model on cuda\n","Epoch 1, Train Loss: 1.87511, Val Loss: 1.73796, Val Acc: 37.180%\n","Epoch 2, Train Loss: 1.66859, Val Loss: 1.62302, Val Acc: 41.230%\n","Epoch 3, Train Loss: 1.54994, Val Loss: 1.53719, Val Acc: 44.910%\n","Epoch 4, Train Loss: 1.45361, Val Loss: 1.48225, Val Acc: 46.840%\n","Epoch 5, Train Loss: 1.38119, Val Loss: 1.42386, Val Acc: 49.690%\n","Epoch 6, Train Loss: 1.32460, Val Loss: 1.43363, Val Acc: 49.020%\n","Epoch 7, Train Loss: 1.27134, Val Loss: 1.40748, Val Acc: 50.580%\n","Epoch 8, Train Loss: 1.24130, Val Loss: 1.40079, Val Acc: 50.970%\n","Epoch 9, Train Loss: 1.18027, Val Loss: 1.39076, Val Acc: 51.540%\n","Epoch 10, Train Loss: 1.15206, Val Loss: 1.40475, Val Acc: 51.570%\n","save result!\n","{'train_losses': [1.8751092349426657, 1.66859199578249, 1.5499362432504002, 1.4536116817329503, 1.3811924050125894, 1.3246046515959728, 1.271341803707654, 1.241298778147637, 1.1802658778202684, 1.152062263669847], 'val_losses': [1.7379580795764924, 1.623017817735672, 1.5371934115886687, 1.4822514653205872, 1.423856109380722, 1.4336288928985597, 1.4074806869029999, 1.400792020559311, 1.3907552063465118, 1.4047533094882965], 'val_accs': [37.18, 41.23, 44.91, 46.839999999999996, 49.69, 49.02, 50.580000000000005, 50.970000000000006, 51.54, 51.57000000000001]}\n","======== End Experiment ========\n","======= Start Experiment ========\n","optim: Adagrad, lr: 0.001\n","model on cuda\n","Epoch 1, Train Loss: 1.79651, Val Loss: 1.65014, Val Acc: 40.500%\n","Epoch 2, Train Loss: 1.55446, Val Loss: 1.53125, Val Acc: 45.380%\n","Epoch 3, Train Loss: 1.44049, Val Loss: 1.47770, Val Acc: 47.610%\n","Epoch 4, Train Loss: 1.36051, Val Loss: 1.43901, Val Acc: 48.780%\n","Epoch 5, Train Loss: 1.28919, Val Loss: 1.42789, Val Acc: 49.610%\n","Epoch 6, Train Loss: 1.21654, Val Loss: 1.40446, Val Acc: 50.460%\n","Epoch 7, Train Loss: 1.17141, Val Loss: 1.39495, Val Acc: 51.320%\n","Epoch 8, Train Loss: 1.13060, Val Loss: 1.40628, Val Acc: 51.150%\n","Epoch 9, Train Loss: 1.07633, Val Loss: 1.39238, Val Acc: 52.090%\n","Epoch 10, Train Loss: 1.02949, Val Loss: 1.42150, Val Acc: 51.830%\n","save result!\n","{'train_losses': [1.7965050875386106, 1.5544574245621887, 1.4404940967318378, 1.3605147072031527, 1.289190385915056, 1.21653911810887, 1.1714134216308594, 1.1305962619902212, 1.076330438444886, 1.0294852430307413], 'val_losses': [1.6501397609710693, 1.5312483072280885, 1.4777047038078308, 1.439014995098114, 1.4278916358947753, 1.404458123445511, 1.394949609041214, 1.4062799870967866, 1.3923822402954102, 1.4214974641799927], 'val_accs': [40.5, 45.379999999999995, 47.61, 48.78, 49.61, 50.46000000000001, 51.32, 51.15, 52.09, 51.83]}\n","======== End Experiment ========\n"]}]},{"cell_type":"markdown","source":["Adam, 0.001"],"metadata":{"id":"AL7VO14vBjQ4"},"id":"AL7VO14vBjQ4"},{"cell_type":"code","source":["seed= 1212\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","\n","parser = argparse.ArgumentParser()\n","args = parser.parse_args(\"\")\n","\n","args.batch_size = 512\n","\n","args.in_dim = 3 * 32 * 32\n","args.out_dim = 10\n","args.hid_dim = 200\n","args.n_layer = 4\n","args.act = 'relu'\n","args.dropout = 0.1\n","args.bn = True\n","\n","args.opt = 'Adam'\n","\n","args.lr = 0.001\n","args.mm = 0.9\n","\n","args.epochs = 10\n","\n","\n","list_var1 = [0.1, 0.2, 0.3, 0.4]        # Dropout\n","list_var2 = [True, False]   # Batch normalization\n","name_var1, name_var2 = 'dropout', 'bn'\n","\n","res = pd.DataFrame(columns=['train_losses', 'val_losses','val_accs'])\n","for var1 in list_var1:\n","    for var2 in list_var2:\n","        print('======= Start Experiment ========')\n","        print('{}: {}, {}: {}'.format(name_var1, var1, name_var2, var2))\n","\n","        args.dropout = var1\n","        args.bn = var2\n","        params, result = experiments(args)\n","        \n","        save_res(result, params, name_var1, name_var2, var1, var2)\n","        print(result)\n","        print('======== End Experiment ========')"],"metadata":{"id":"eaeAIp5iw80o","executionInfo":{"status":"ok","timestamp":1655182876355,"user_tz":-540,"elapsed":289854,"user":{"displayName":"","userId":""}},"outputId":"cb80e90a-e41e-485e-f1cf-6964c2cf14ae","colab":{"base_uri":"https://localhost:8080/"}},"id":"eaeAIp5iw80o","execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["======= Start Experiment ========\n","dropout: 0.1, bn: True\n","model on cuda\n","Epoch 1, Train Loss: 1.81017, Val Loss: 1.63951, Val Acc: 41.030%\n","Epoch 2, Train Loss: 1.55573, Val Loss: 1.53955, Val Acc: 44.760%\n","Epoch 3, Train Loss: 1.44469, Val Loss: 1.46751, Val Acc: 47.400%\n","Epoch 4, Train Loss: 1.35177, Val Loss: 1.45348, Val Acc: 48.970%\n","Epoch 5, Train Loss: 1.28868, Val Loss: 1.42879, Val Acc: 49.870%\n","Epoch 6, Train Loss: 1.22482, Val Loss: 1.40040, Val Acc: 50.420%\n","Epoch 7, Train Loss: 1.17550, Val Loss: 1.43008, Val Acc: 49.720%\n","Epoch 8, Train Loss: 1.14018, Val Loss: 1.40775, Val Acc: 51.200%\n","Epoch 9, Train Loss: 1.09738, Val Loss: 1.41248, Val Acc: 51.320%\n","Epoch 10, Train Loss: 1.05408, Val Loss: 1.44315, Val Acc: 50.690%\n","save result!\n","{'train_losses': [1.8101665702047227, 1.5557341469994075, 1.444689552995223, 1.3517654621148412, 1.288682122773762, 1.2248163283625735, 1.1755028616023968, 1.1401806330379052, 1.097380401967447, 1.0540775256820871], 'val_losses': [1.639512026309967, 1.5395547688007354, 1.4675077617168426, 1.4534813106060027, 1.4287915527820587, 1.400398051738739, 1.4300839960575105, 1.4077532529830932, 1.4124816000461577, 1.4431514263153076], 'val_accs': [41.03, 44.76, 47.4, 48.97, 49.87, 50.42, 49.72, 51.2, 51.32, 50.690000000000005]}\n","======== End Experiment ========\n","======= Start Experiment ========\n","dropout: 0.1, bn: False\n","model on cuda\n","Epoch 1, Train Loss: 1.87852, Val Loss: 1.71002, Val Acc: 39.460%\n","Epoch 2, Train Loss: 1.62300, Val Loss: 1.57792, Val Acc: 43.750%\n","Epoch 3, Train Loss: 1.50496, Val Loss: 1.54305, Val Acc: 45.190%\n","Epoch 4, Train Loss: 1.41954, Val Loss: 1.47805, Val Acc: 48.000%\n","Epoch 5, Train Loss: 1.34195, Val Loss: 1.44140, Val Acc: 49.230%\n","Epoch 6, Train Loss: 1.26543, Val Loss: 1.43964, Val Acc: 50.370%\n","Epoch 7, Train Loss: 1.21385, Val Loss: 1.42124, Val Acc: 50.230%\n","Epoch 8, Train Loss: 1.16161, Val Loss: 1.42452, Val Acc: 49.860%\n","Epoch 9, Train Loss: 1.10512, Val Loss: 1.42921, Val Acc: 50.950%\n","Epoch 10, Train Loss: 1.05761, Val Loss: 1.41487, Val Acc: 51.920%\n","save result!\n","{'train_losses': [1.8785231520857992, 1.6230032911783532, 1.5049629875376254, 1.4195367973062056, 1.3419484081147592, 1.265430670750292, 1.213854877254631, 1.161610194399387, 1.1051191468782062, 1.0576070780995526], 'val_losses': [1.7100230157375336, 1.5779198050498962, 1.5430489242076875, 1.4780487537384033, 1.441398936510086, 1.439636915922165, 1.42123863697052, 1.4245150446891786, 1.4292085826396943, 1.4148731410503388], 'val_accs': [39.46, 43.75, 45.190000000000005, 48.0, 49.230000000000004, 50.370000000000005, 50.23, 49.86, 50.949999999999996, 51.92]}\n","======== End Experiment ========\n","======= Start Experiment ========\n","dropout: 0.2, bn: True\n","model on cuda\n","Epoch 1, Train Loss: 1.88891, Val Loss: 1.71933, Val Acc: 37.790%\n","Epoch 2, Train Loss: 1.63600, Val Loss: 1.59017, Val Acc: 42.840%\n","Epoch 3, Train Loss: 1.51755, Val Loss: 1.53277, Val Acc: 44.810%\n","Epoch 4, Train Loss: 1.43509, Val Loss: 1.49637, Val Acc: 47.240%\n","Epoch 5, Train Loss: 1.36545, Val Loss: 1.47518, Val Acc: 48.430%\n","Epoch 6, Train Loss: 1.31064, Val Loss: 1.44592, Val Acc: 48.380%\n","Epoch 7, Train Loss: 1.25142, Val Loss: 1.43850, Val Acc: 49.150%\n","Epoch 8, Train Loss: 1.21601, Val Loss: 1.42194, Val Acc: 50.910%\n","Epoch 9, Train Loss: 1.17948, Val Loss: 1.42667, Val Acc: 50.610%\n","Epoch 10, Train Loss: 1.13991, Val Loss: 1.43550, Val Acc: 50.280%\n","save result!\n","{'train_losses': [1.888911632042897, 1.63599579545516, 1.5175458192825317, 1.4350873518593703, 1.3654460016685197, 1.3106432383573507, 1.2514237452156936, 1.2160078151316582, 1.179477174070817, 1.139906260031688], 'val_losses': [1.7193283557891845, 1.5901713430881501, 1.532774430513382, 1.4963717222213746, 1.4751754105091095, 1.445922613143921, 1.4384983599185943, 1.421944898366928, 1.4266656816005707, 1.4354993164539338], 'val_accs': [37.79, 42.84, 44.81, 47.24, 48.43, 48.38, 49.15, 50.91, 50.61, 50.28]}\n","======== End Experiment ========\n","======= Start Experiment ========\n","dropout: 0.2, bn: False\n","model on cuda\n","Epoch 1, Train Loss: 1.90315, Val Loss: 1.75402, Val Acc: 37.260%\n","Epoch 2, Train Loss: 1.65065, Val Loss: 1.61664, Val Acc: 42.780%\n","Epoch 3, Train Loss: 1.53639, Val Loss: 1.54610, Val Acc: 45.480%\n","Epoch 4, Train Loss: 1.45014, Val Loss: 1.50241, Val Acc: 47.270%\n","Epoch 5, Train Loss: 1.39191, Val Loss: 1.48010, Val Acc: 48.380%\n","Epoch 6, Train Loss: 1.32421, Val Loss: 1.45408, Val Acc: 49.210%\n","Epoch 7, Train Loss: 1.27626, Val Loss: 1.46131, Val Acc: 49.280%\n","Epoch 8, Train Loss: 1.22895, Val Loss: 1.44696, Val Acc: 49.590%\n","Epoch 9, Train Loss: 1.18122, Val Loss: 1.45634, Val Acc: 49.290%\n","Epoch 10, Train Loss: 1.14154, Val Loss: 1.42565, Val Acc: 50.610%\n","save result!\n","{'train_losses': [1.903145731249942, 1.6506454507006874, 1.5363904343375676, 1.4501447405996202, 1.3919149275067486, 1.32421122925191, 1.2762565522254268, 1.2289518615867518, 1.181221678287168, 1.1415406126010268], 'val_losses': [1.754022091627121, 1.6166364192962646, 1.5460992276668548, 1.5024137437343597, 1.4800971627235413, 1.4540839850902558, 1.461314332485199, 1.446963232755661, 1.456335175037384, 1.4256548404693603], 'val_accs': [37.26, 42.78, 45.48, 47.27, 48.38, 49.21, 49.28, 49.59, 49.29, 50.61]}\n","======== End Experiment ========\n","======= Start Experiment ========\n","dropout: 0.3, bn: True\n","model on cuda\n","Epoch 1, Train Loss: 1.95167, Val Loss: 1.77510, Val Acc: 36.410%\n","Epoch 2, Train Loss: 1.69511, Val Loss: 1.64718, Val Acc: 41.290%\n","Epoch 3, Train Loss: 1.57870, Val Loss: 1.56528, Val Acc: 45.150%\n","Epoch 4, Train Loss: 1.49494, Val Loss: 1.55058, Val Acc: 44.900%\n","Epoch 5, Train Loss: 1.43851, Val Loss: 1.49685, Val Acc: 47.260%\n","Epoch 6, Train Loss: 1.37161, Val Loss: 1.46796, Val Acc: 48.640%\n","Epoch 7, Train Loss: 1.32552, Val Loss: 1.45127, Val Acc: 48.770%\n","Epoch 8, Train Loss: 1.28706, Val Loss: 1.44008, Val Acc: 49.350%\n","Epoch 9, Train Loss: 1.24340, Val Loss: 1.45139, Val Acc: 49.890%\n","Epoch 10, Train Loss: 1.20941, Val Loss: 1.44286, Val Acc: 49.560%\n","save result!\n","{'train_losses': [1.951674079593224, 1.695111493521099, 1.578702510157718, 1.4949424191366267, 1.438513785977907, 1.3716084112094928, 1.3255215098586264, 1.2870614015603368, 1.2434005676945554, 1.2094093983686423], 'val_losses': [1.775095373392105, 1.6471825122833252, 1.5652787327766418, 1.5505761742591857, 1.496846330165863, 1.467956018447876, 1.4512664020061492, 1.4400824069976808, 1.451392650604248, 1.4428629398345947], 'val_accs': [36.41, 41.29, 45.15, 44.9, 47.260000000000005, 48.64, 48.77, 49.35, 49.89, 49.559999999999995]}\n","======== End Experiment ========\n","======= Start Experiment ========\n","dropout: 0.3, bn: False\n","model on cuda\n","Epoch 1, Train Loss: 1.95956, Val Loss: 1.78147, Val Acc: 35.230%\n","Epoch 2, Train Loss: 1.70630, Val Loss: 1.66849, Val Acc: 40.550%\n","Epoch 3, Train Loss: 1.58855, Val Loss: 1.60372, Val Acc: 43.070%\n","Epoch 4, Train Loss: 1.51716, Val Loss: 1.53515, Val Acc: 46.130%\n","Epoch 5, Train Loss: 1.44040, Val Loss: 1.51789, Val Acc: 46.650%\n","Epoch 6, Train Loss: 1.38210, Val Loss: 1.48886, Val Acc: 47.920%\n","Epoch 7, Train Loss: 1.32638, Val Loss: 1.47967, Val Acc: 48.570%\n","Epoch 8, Train Loss: 1.28888, Val Loss: 1.46845, Val Acc: 48.460%\n","Epoch 9, Train Loss: 1.24718, Val Loss: 1.47478, Val Acc: 49.290%\n","Epoch 10, Train Loss: 1.20715, Val Loss: 1.47195, Val Acc: 49.780%\n","save result!\n","{'train_losses': [1.9595560273037682, 1.7062975605831872, 1.5885547158084339, 1.5171630955949615, 1.4403998036927814, 1.382095970684969, 1.3263778988319108, 1.2888795092136045, 1.2471755184704745, 1.2071505317205116], 'val_losses': [1.7814704954624176, 1.6684907793998718, 1.6037169635295867, 1.5351536452770234, 1.5178926169872284, 1.4888619780540466, 1.4796681165695191, 1.4684541404247284, 1.4747787356376647, 1.4719478070735932], 'val_accs': [35.23, 40.550000000000004, 43.07, 46.129999999999995, 46.650000000000006, 47.92, 48.57, 48.46, 49.29, 49.78]}\n","======== End Experiment ========\n","======= Start Experiment ========\n","dropout: 0.4, bn: True\n","model on cuda\n","Epoch 1, Train Loss: 2.03450, Val Loss: 1.84514, Val Acc: 33.450%\n","Epoch 2, Train Loss: 1.76572, Val Loss: 1.71240, Val Acc: 37.950%\n","Epoch 3, Train Loss: 1.65088, Val Loss: 1.62762, Val Acc: 42.010%\n","Epoch 4, Train Loss: 1.56381, Val Loss: 1.58539, Val Acc: 43.780%\n","Epoch 5, Train Loss: 1.50147, Val Loss: 1.53627, Val Acc: 45.460%\n","Epoch 6, Train Loss: 1.45060, Val Loss: 1.49881, Val Acc: 47.140%\n","Epoch 7, Train Loss: 1.40425, Val Loss: 1.48686, Val Acc: 47.530%\n","Epoch 8, Train Loss: 1.36537, Val Loss: 1.47512, Val Acc: 48.440%\n","Epoch 9, Train Loss: 1.32573, Val Loss: 1.47640, Val Acc: 48.540%\n","Epoch 10, Train Loss: 1.29267, Val Loss: 1.45712, Val Acc: 49.360%\n","save result!\n","{'train_losses': [2.034504329101949, 1.7657246755648264, 1.6508797859843773, 1.5638114892983739, 1.5014702836169471, 1.4506022432182408, 1.4042465581169612, 1.3653747492198702, 1.3257335092447982, 1.292666737037369], 'val_losses': [1.8451390981674194, 1.712395691871643, 1.627622479200363, 1.5853878140449524, 1.5362747192382813, 1.4988058030605316, 1.4868575930595398, 1.475123977661133, 1.4763989984989165, 1.4571247756481172], 'val_accs': [33.45, 37.95, 42.01, 43.78, 45.46, 47.14, 47.53, 48.44, 48.54, 49.36]}\n","======== End Experiment ========\n","======= Start Experiment ========\n","dropout: 0.4, bn: False\n","model on cuda\n","Epoch 1, Train Loss: 2.00458, Val Loss: 1.84590, Val Acc: 32.030%\n","Epoch 2, Train Loss: 1.76867, Val Loss: 1.71093, Val Acc: 38.960%\n","Epoch 3, Train Loss: 1.64181, Val Loss: 1.64325, Val Acc: 41.880%\n","Epoch 4, Train Loss: 1.56465, Val Loss: 1.59234, Val Acc: 44.470%\n","Epoch 5, Train Loss: 1.49773, Val Loss: 1.56831, Val Acc: 45.120%\n","Epoch 6, Train Loss: 1.45457, Val Loss: 1.53899, Val Acc: 46.210%\n","Epoch 7, Train Loss: 1.40975, Val Loss: 1.51859, Val Acc: 46.830%\n","Epoch 8, Train Loss: 1.35787, Val Loss: 1.52716, Val Acc: 47.160%\n","Epoch 9, Train Loss: 1.32759, Val Loss: 1.52543, Val Acc: 48.200%\n","Epoch 10, Train Loss: 1.30128, Val Loss: 1.51459, Val Acc: 47.990%\n","save result!\n","{'train_losses': [2.0045752314072622, 1.7686658614798436, 1.641810806491707, 1.5646513474138477, 1.4977337846273109, 1.4545743601231635, 1.4097452390043042, 1.357869963102703, 1.3275923759122439, 1.301275281966487], 'val_losses': [1.8458954095840454, 1.7109327435493469, 1.6432460248470306, 1.5923384487628938, 1.5683113753795623, 1.538987672328949, 1.5185906052589417, 1.5271644711494445, 1.5254310429096223, 1.5145899295806884], 'val_accs': [32.029999999999994, 38.96, 41.88, 44.47, 45.12, 46.21, 46.83, 47.160000000000004, 48.199999999999996, 47.99]}\n","======== End Experiment ========\n"]}]},{"cell_type":"code","source":["seed= 1212\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","\n","parser = argparse.ArgumentParser()\n","args = parser.parse_args(\"\")\n","\n","args.batch_size = 512\n","\n","args.in_dim = 3 * 32 * 32\n","args.out_dim = 10\n","args.hid_dim = 200\n","args.n_layer = 4\n","args.act = 'relu'\n","args.dropout = 0.1\n","args.bn = False\n","\n","args.opt = 'Adam'\n","\n","args.lr = 0.001\n","args.mm = 0.9\n","\n","args.epochs = 20\n","\n","\n","params, result = experiments(args)\n","print(result)"],"metadata":{"id":"OyVO5NVCYpje","executionInfo":{"status":"ok","timestamp":1655183281344,"user_tz":-540,"elapsed":183943,"user":{"displayName":"","userId":""}},"outputId":"303648fb-28e1-464f-9cfd-1ee5aa12143e","colab":{"base_uri":"https://localhost:8080/"}},"id":"OyVO5NVCYpje","execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["model on cuda\n","Epoch 1, Train Loss: 1.88170, Val Loss: 1.69411, Val Acc: 40.130%\n","Epoch 2, Train Loss: 1.61635, Val Loss: 1.58601, Val Acc: 43.800%\n","Epoch 3, Train Loss: 1.51184, Val Loss: 1.52016, Val Acc: 46.050%\n","Epoch 4, Train Loss: 1.40767, Val Loss: 1.45922, Val Acc: 48.180%\n","Epoch 5, Train Loss: 1.34029, Val Loss: 1.44013, Val Acc: 50.110%\n","Epoch 6, Train Loss: 1.26686, Val Loss: 1.43870, Val Acc: 49.680%\n","Epoch 7, Train Loss: 1.21391, Val Loss: 1.40657, Val Acc: 51.520%\n","Epoch 8, Train Loss: 1.16020, Val Loss: 1.41465, Val Acc: 51.420%\n","Epoch 9, Train Loss: 1.11530, Val Loss: 1.41424, Val Acc: 51.480%\n","Epoch 10, Train Loss: 1.06478, Val Loss: 1.45037, Val Acc: 50.810%\n","Epoch 11, Train Loss: 1.00963, Val Loss: 1.45155, Val Acc: 51.550%\n","Epoch 12, Train Loss: 0.96715, Val Loss: 1.45925, Val Acc: 51.940%\n","Epoch 13, Train Loss: 0.92691, Val Loss: 1.50373, Val Acc: 51.400%\n","Epoch 14, Train Loss: 0.88328, Val Loss: 1.52428, Val Acc: 52.170%\n","Epoch 15, Train Loss: 0.84202, Val Loss: 1.53716, Val Acc: 52.420%\n","Epoch 16, Train Loss: 0.81138, Val Loss: 1.53851, Val Acc: 51.250%\n","Epoch 17, Train Loss: 0.77988, Val Loss: 1.59768, Val Acc: 52.020%\n","Epoch 18, Train Loss: 0.75269, Val Loss: 1.61602, Val Acc: 52.310%\n","Epoch 19, Train Loss: 0.71053, Val Loss: 1.64212, Val Acc: 51.240%\n","Epoch 20, Train Loss: 0.69642, Val Loss: 1.68075, Val Acc: 51.650%\n","{'train_losses': [1.881697879561895, 1.6163524434536318, 1.5118427880202667, 1.4076719163339348, 1.3402933953683587, 1.2668606171125099, 1.2139148093477081, 1.16020212445078, 1.11530348847184, 1.064777732649936, 1.009629284279256, 0.967147296742548, 0.9269136680832392, 0.8832847317562827, 0.8420172529884532, 0.8113750453236737, 0.779879105996482, 0.7526881551440758, 0.7105269779132891, 0.6964229360411439], 'val_losses': [1.6941084027290345, 1.5860096275806428, 1.5201563775539397, 1.4592248260974885, 1.4401332318782807, 1.4386987030506133, 1.4065654158592225, 1.4146467924118042, 1.4142359018325805, 1.4503741025924684, 1.451552551984787, 1.4592525362968445, 1.5037263870239257, 1.5242800176143647, 1.5371590912342072, 1.5385103523731232, 1.597683048248291, 1.6160206615924835, 1.6421199023723603, 1.6807475984096527], 'val_accs': [40.129999999999995, 43.8, 46.050000000000004, 48.18, 50.11, 49.68, 51.519999999999996, 51.42, 51.480000000000004, 50.81, 51.55, 51.94, 51.4, 52.17, 52.42, 51.24999999999999, 52.019999999999996, 52.31, 51.239999999999995, 51.65]}\n"]}]},{"cell_type":"markdown","source":["- Epoch수가 증가함에 따라, Train Loss는 낮아지나 Val Loss는 다시 증가하는 양상을 보임 -> Overfit\n","- 일반화를 위한 실험 다시 필요 "],"metadata":{"id":"xh5gihJHnCBM"},"id":"xh5gihJHnCBM"}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"colab":{"name":"2_2_CIFAR10_Hyperparameter_tuning.ipynb의 사본","provenance":[{"file_id":"https://github.com/haesung-j/DeepLearning/blob/main/SADL/2_2_CIFAR10_Hyperparameter_tuning.ipynb","timestamp":1655183314516}]},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"79b0e49828f24d35a862f07acfe5234d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_619fbcbee8b34a2c9d081693cd50cee5","IPY_MODEL_6cdb920cc0864a27ac6a6257357aef30","IPY_MODEL_e58f1dd1fd3b4988aa71920517cce315"],"layout":"IPY_MODEL_3825bfd0b59443c3bd7aac7b09a42c73"}},"619fbcbee8b34a2c9d081693cd50cee5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_41a33beb3ce142889727af4cfa609422","placeholder":"​","style":"IPY_MODEL_71d5512686eb4d4d9d38a34bcc4cb66f","value":""}},"6cdb920cc0864a27ac6a6257357aef30":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c969909865ef4c08add96692fe560f4c","max":170498071,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ab48b13cbd6a4b56aa49f54f1efa2a02","value":170498071}},"e58f1dd1fd3b4988aa71920517cce315":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d9368458619f491e9e1bacce1acb6606","placeholder":"​","style":"IPY_MODEL_e743ae625619424c8c713694f5bf7931","value":" 170499072/? [00:05&lt;00:00, 26111949.13it/s]"}},"3825bfd0b59443c3bd7aac7b09a42c73":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"41a33beb3ce142889727af4cfa609422":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"71d5512686eb4d4d9d38a34bcc4cb66f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c969909865ef4c08add96692fe560f4c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab48b13cbd6a4b56aa49f54f1efa2a02":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d9368458619f491e9e1bacce1acb6606":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e743ae625619424c8c713694f5bf7931":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}
